{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0dc998-38be-4947-8b19-060551184e31",
   "metadata": {},
   "source": [
    "### Deep Neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2468039-1512-441b-b06c-86d48df27f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a9558b-e6e8-4ab3-a089-3c50b025b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acitvation functions\n",
    "def sigmoid(z): #sigmoid activation function\n",
    "    A = 1/(1+np.exp(-z))\n",
    "    return A\n",
    "\n",
    "def softmax(z): #softmax activation function\n",
    "    expz = np.exp(z)\n",
    "    return expz/(np.sum(expz,0))\n",
    "\n",
    "def relu(z):  #ReLU activation function\n",
    "    A = np.maximum(0,z)\n",
    "    return A\n",
    "\n",
    "def tanh(x): #tanh activation function\n",
    "    return np.tanh(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6742cfda-5cd3-455c-8a0b-e1ebeb73a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation for a single layer\n",
    "def forward_propagation(input_data, weights, biases, activation_function):\n",
    "    z = np.dot(weights, input_data) + biases  # Linear combination of weights and input\n",
    "    # Selecting the chosen activation function\n",
    "    if activation_function == \"relu\":\n",
    "        a = relu(z)\n",
    "    elif activation_function == \"sigmoid\":\n",
    "        a = sigmoid(z)\n",
    "    elif activation_function == \"tanh\":\n",
    "        a = tanh(z)\n",
    "    elif activation_function == \"softmax\":\n",
    "        a = softmax(z)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function.\")  # Error for invalid input\n",
    "    return a, z  # Return both activation output and pre-activation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede54dd2-51f2-4a9b-9ae4-3e0ed65b4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing the number of learnable parameters\n",
    "def compute_parameters(neurons, input_neurons):\n",
    "    parameters = neurons * input_neurons  # Parameters for weights\n",
    "    parameters += neurons  # Adding biases\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ba3a2d-a7ed-4393-8630-ff334884eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating a hidden layer with activation\n",
    "def define_hidden_layer_with_activation(input_neurons):\n",
    "    neurons = int(input(f\"Enter the number of neurons for the hidden layer: \")) #user input for number of neurons for the hidden layer\n",
    "    activation = input(\"Choose activation function (relu, sigmoid, tanh): \").lower() #user input for selecting the activation function for hidden layer\n",
    "\n",
    "    # Initializing weights and biases for the layer\n",
    "    weights = np.random.randn(neurons, input_neurons) * 0.01  # Small random weights are initialized by scaling them for avoiding the instability during the training\n",
    "    biases = np.zeros((neurons, 1))  # Biases initialized to zero\n",
    "\n",
    "    # Calculating the number of learnable parameters\n",
    "    num_of_params = compute_parameters(neurons, input_neurons)\n",
    "\n",
    "    return neurons, weights, biases, activation  # Return hidden layer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57b3943-bfa8-4505-b2c8-4a8f1b4dda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of input features:  10\n",
      "Enter the number of hidden layers:  3\n",
      "Enter the number of neurons for the hidden layer:  8\n",
      "Choose activation function (relu, sigmoid, tanh):  relu\n",
      "Enter the number of neurons for the hidden layer:  10\n",
      "Choose activation function (relu, sigmoid, tanh):  relu\n",
      "Enter the number of neurons for the hidden layer:  12\n",
      "Choose activation function (relu, sigmoid, tanh):  relu\n",
      "Enter the number of output neurons:  2\n",
      "Choose activation function for the output layer (relu, sigmoid, softmax):  softmax\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of the Neural Network:\n",
      "Layer 1: 8 neurons, activation: relu, parameters: 88\n",
      "Layer 2: 10 neurons, activation: relu, parameters: 90\n",
      "Layer 3: 12 neurons, activation: relu, parameters: 132\n",
      "Layer 4: 2 neurons, activation: softmax, parameters: 26\n",
      "\n",
      "Total learnable parameters in the network: 336\n",
      "\n",
      "Performing forward propagation with random input:\n",
      "Layer 1 forward pass:\n",
      "Output shape: (8, 1)\n",
      "Layer 2 forward pass:\n",
      "Output shape: (10, 1)\n",
      "Layer 3 forward pass:\n",
      "Output shape: (12, 1)\n",
      "Layer 4 forward pass:\n",
      "Output shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Function for designing the neural network with user inputs\n",
    "def design_of_neural_network():\n",
    "   \n",
    "    input_neurons = int(input(\"Enter the number of input features: \")) #user input for number of neurons in the input layer\n",
    "    num_of_hidden_layers = int(input(\"Enter the number of hidden layers: \")) #user input for number of hidden layers\n",
    "\n",
    "    layers = []  # List to store layers\n",
    "    previous_neurons = input_neurons  # Assigning a new variable for Keeping track of the previous layer's size\n",
    "\n",
    "    for i in range(num_of_hidden_layers):\n",
    "        # Define a hidden layer and append to the layers list\n",
    "        neurons, weights, biases, activation = define_hidden_layer_with_activation(previous_neurons)\n",
    "        layers.append((weights, biases, activation))\n",
    "        previous_neurons = neurons  # Update for the next layer\n",
    "\n",
    "    \n",
    "    output_neurons = int(input(\"Enter the number of output neurons: \")) #user input for the number of output neurons\n",
    "    output_activation = input(\"Choose activation function for the output layer (relu, sigmoid, softmax): \").lower() #user input for selecting the activation function for output layer\n",
    "\n",
    "    # Initialize weights and biases for the output layer\n",
    "    output_weights = np.random.randn(output_neurons, previous_neurons) * 0.01  # Small random weights\n",
    "    output_biases = np.zeros((output_neurons, 1))  # Biases initialized to zero\n",
    "\n",
    "    # Calculate the number of learnable parameters for the output layer\n",
    "    num_of_params = compute_parameters(output_neurons, previous_neurons)\n",
    "\n",
    "    # Append the output layer to the layer's list\n",
    "    layers.append((output_weights, output_biases, output_activation))\n",
    "\n",
    "    print(\"\\nSummary of the Neural Network:\")\n",
    "    total_params = 0  # Initialize total parameter count\n",
    "    # Loop through layers to print their summary\n",
    "    for i, (weights, biases, activation) in enumerate(layers):\n",
    "        params = weights.size + biases.size  # Compute total parameters in the layer\n",
    "        print(f\"Layer {i + 1}: {weights.shape[0]} neurons, activation: {activation}, parameters: {params}\")\n",
    "        total_params += params  # Add to the total count\n",
    "\n",
    "    print(f\"\\nTotal learnable parameters in the network: {total_params}\")\n",
    "\n",
    "    \n",
    "    print(\"\\nPerforming forward propagation with random input:\")\n",
    "    input_data = np.random.randn(input_neurons, 1)  # Generate random input data\n",
    "    for i, (weights, biases, activation) in enumerate(layers):\n",
    "        print(f\"Layer {i + 1} forward pass:\")\n",
    "        input_data, z = forward_propagation(input_data, weights, biases, activation)  # Forward pass\n",
    "        print(f\"Output shape: {input_data.shape}\")  # Print output shape of the layer\n",
    "\n",
    "\n",
    "design_of_neural_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5b5ac-1ee1-4ac1-92e1-a4cedfd30eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
